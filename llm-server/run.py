#!/usr/bin/env python3
"""
LLM Server ì‹¤í–‰ ìŠ¤í¬ë¦½íŠ¸
"""

import os
import sys
import logging
from pathlib import Path

# í™˜ê²½ë³€ìˆ˜ ë¡œë“œ
try:
    from dotenv import load_dotenv
    load_dotenv()
    print("âœ… .env íŒŒì¼ ë¡œë“œë¨")
except ImportError:
    print("âš ï¸ python-dotenvê°€ ì„¤ì¹˜ë˜ì§€ ì•ŠìŒ. pip install python-dotenv")

# ë¡œê¹… ì„¤ì •
logging.basicConfig(
    level=getattr(logging, os.getenv('LOG_LEVEL', 'INFO')),
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

def check_gpu():
    """GPU í™˜ê²½ í™•ì¸"""
    try:
        import torch
        
        print(f"ğŸ” CUDA ì‚¬ìš© ê°€ëŠ¥: {torch.cuda.is_available()}")
        if torch.cuda.is_available():
            print(f"ğŸ® GPU ê°œìˆ˜: {torch.cuda.device_count()}")
            print(f"ğŸ® í˜„ì¬ GPU: {torch.cuda.current_device()}")
            print(f"ğŸ® GPU ì´ë¦„: {torch.cuda.get_device_name()}")
            
            # GPU ë©”ëª¨ë¦¬ í™•ì¸
            for i in range(torch.cuda.device_count()):
                props = torch.cuda.get_device_properties(i)
                memory_gb = props.total_memory / 1024**3
                print(f"ğŸ® GPU {i}: {props.name}, ë©”ëª¨ë¦¬: {memory_gb:.1f}GB")
        else:
            print("âš ï¸ CUDAë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. CPUë¡œ ì‹¤í–‰ë©ë‹ˆë‹¤.")
            
    except ImportError:
        print("âŒ PyTorchê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        return False
        
    return True

def check_model_cache():
    """ëª¨ë¸ ìºì‹œ ë””ë ‰í„°ë¦¬ í™•ì¸"""
    cache_dir = Path(os.getenv('MODEL_CACHE_DIR', './model_cache'))
    
    if not cache_dir.exists():
        print(f"ğŸ“ ëª¨ë¸ ìºì‹œ ë””ë ‰í„°ë¦¬ ìƒì„±: {cache_dir}")
        cache_dir.mkdir(parents=True, exist_ok=True)
    else:
        print(f"ğŸ“ ëª¨ë¸ ìºì‹œ ë””ë ‰í„°ë¦¬ ì¡´ì¬: {cache_dir}")
    
    return cache_dir

def pre_download_model():
    """ëª¨ë¸ ì‚¬ì „ ë‹¤ìš´ë¡œë“œ (ì„ íƒì‚¬í•­)"""
    model_name = os.getenv('MODEL_NAME', 'LGAI-EXAONE/EXAONE-3.5-7.8B-Instruct')
    
    try:
        from transformers import AutoTokenizer, AutoModelForCausalLM
        import torch
        
        print(f"ğŸ“¥ ëª¨ë¸ ì‚¬ì „ ë‹¤ìš´ë¡œë“œ ì‹œì‘: {model_name}")
        
        # í† í¬ë‚˜ì´ì € ë‹¤ìš´ë¡œë“œ
        print("ğŸ“¥ í† í¬ë‚˜ì´ì € ë‹¤ìš´ë¡œë“œ ì¤‘...")
        tokenizer = AutoTokenizer.from_pretrained(
            model_name,
            cache_dir=os.getenv('MODEL_CACHE_DIR', './model_cache')
        )
        
        # ëª¨ë¸ ë‹¤ìš´ë¡œë“œ (ì‹œê°„ì´ ì˜¤ë˜ ê±¸ë¦´ ìˆ˜ ìˆìŒ)
        print("ğŸ“¥ ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ì¤‘... (ì‹œê°„ì´ ì˜¤ë˜ ê±¸ë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤)")
        model = AutoModelForCausalLM.from_pretrained(
            model_name,
            torch_dtype=getattr(torch, os.getenv('TORCH_DTYPE', 'bfloat16')),
            trust_remote_code=True,
            device_map=os.getenv('DEVICE_MAP', 'auto'),
            cache_dir=os.getenv('MODEL_CACHE_DIR', './model_cache'),
            low_cpu_mem_usage=os.getenv('LOW_CPU_MEM_USAGE', 'true').lower() == 'true'
        )
        
        print("âœ… ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ì™„ë£Œ!")
        
        # ë©”ëª¨ë¦¬ ì •ë¦¬
        del model
        del tokenizer
        
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
            
        return True
        
    except Exception as e:
        print(f"âŒ ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨: {e}")
        return False

def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    print("ğŸš€ OpusCine LLM Server ì‹œì‘...")
    print("=" * 50)
    
    # í™˜ê²½ ì •ë³´ ì¶œë ¥
    print(f"ğŸ“ Python ë²„ì „: {sys.version}")
    print(f"ğŸ“ ì‘ì—… ë””ë ‰í„°ë¦¬: {os.getcwd()}")
    print(f"ğŸ“ ëª¨ë¸: {os.getenv('MODEL_NAME', 'LGAI-EXAONE/EXAONE-3.5-7.8B-Instruct')}")
    print(f"ğŸ“ í¬íŠ¸: {os.getenv('PORT', '8001')}")
    print()
    
    # GPU í™˜ê²½ í™•ì¸
    gpu_available = check_gpu()
    print()
    
    # ëª¨ë¸ ìºì‹œ ë””ë ‰í„°ë¦¬ í™•ì¸
    cache_dir = check_model_cache()
    print()
    
    # ì‚¬ì „ ë‹¤ìš´ë¡œë“œ ì—¬ë¶€ í™•ì¸
    if os.getenv('USE_CACHE', 'true').lower() == 'true':
        print("ğŸ¤” ëª¨ë¸ì„ ì‚¬ì „ ë‹¤ìš´ë¡œë“œí•˜ì‹œê² ìŠµë‹ˆê¹Œ? (ì‹œê°„ì´ ë§ì´ ê±¸ë¦½ë‹ˆë‹¤)")
        choice = input("ì‚¬ì „ ë‹¤ìš´ë¡œë“œ? [y/N]: ").lower().strip()
        
        if choice in ['y', 'yes']:
            pre_download_model()
        else:
            print("â­ï¸ ì‚¬ì „ ë‹¤ìš´ë¡œë“œ ê±´ë„ˆë›°ê¸° (ì„œë²„ ì‹œì‘ ì‹œ ë‹¤ìš´ë¡œë“œë©ë‹ˆë‹¤)")
    
    print()
    print("ğŸ¬ FastAPI ì„œë²„ ì‹œì‘...")
    
    # FastAPI ì„œë²„ ì‹¤í–‰
    import uvicorn
    
    uvicorn.run(
        "app:app",
        host=os.getenv('HOST', '0.0.0.0'),
        port=int(os.getenv('PORT', 8001)),
        log_level=os.getenv('LOG_LEVEL', 'info').lower(),
        reload=os.getenv('DEBUG', 'false').lower() == 'true'
    )

if __name__ == "__main__":
    main() 