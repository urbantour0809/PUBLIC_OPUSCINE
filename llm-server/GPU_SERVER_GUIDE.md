# ğŸ–¥ï¸ OpusCine LLM Linux GPU ì„œë²„ ì„¤ì • ê°€ì´ë“œ

## ğŸ“‹ ì‹œìŠ¤í…œ ìš”êµ¬ì‚¬í•­

### í•˜ë“œì›¨ì–´
- **GPU**: NVIDIA GPU (16GB+ VRAM ê¶Œì¥)
- **RAM**: 32GB+ ê¶Œì¥
- **Storage**: 50GB+ ì—¬ìœ  ê³µê°„

### ì†Œí”„íŠ¸ì›¨ì–´
- **OS**: Linux (WSL ì§€ì›)
- **Python**: 3.8+
- **CUDA**: 11.8+
- **Anaconda**: ì„¤ì¹˜ë¨ (gpu í™˜ê²½ í™œì„±í™”)

## ğŸš€ ë¹ ë¥¸ ì„¤ì • (WSL í™˜ê²½)

### 1ë‹¨ê³„: WSL ë° conda í™˜ê²½ ì¤€ë¹„
```bash
# WSL ì‹¤í–‰
wsl

# conda gpu í™˜ê²½ í™œì„±í™”
conda activate gpu

# í”„ë¡œì íŠ¸ ë””ë ‰í„°ë¦¬ë¡œ ì´ë™
cd /path/to/llm-server
```

### 2ë‹¨ê³„: ì„œë²„ í™˜ê²½ ì„¤ì •
```bash
# ì„¤ì • ìŠ¤í¬ë¦½íŠ¸ ì‹¤í–‰ ê¶Œí•œ ë¶€ì—¬
chmod +x gpu_server_setup.sh start_llm_server.sh setup_ngrok.sh

# GPU ì„œë²„ í™˜ê²½ ì„¤ì • ì‹¤í–‰
./gpu_server_setup.sh
```

### 3ë‹¨ê³„: ngrok í† í° ì„¤ì •
```bash
# ngrok ê³„ì •ì—ì„œ í† í° íšë“ í›„ ì„¤ì •
ngrok config add-authtoken YOUR_NGROK_TOKEN
```

### 4ë‹¨ê³„: ì„œë²„ ì‹œì‘
```bash
# í„°ë¯¸ë„ 1: LLM ì„œë²„ ì‹œì‘
./start_llm_server.sh

# í„°ë¯¸ë„ 2: ngrok í„°ë„ ì‹œì‘ (ìƒˆ í„°ë¯¸ë„ì—ì„œ)
./setup_ngrok.sh
```

## ğŸ”§ ìƒì„¸ ì„¤ì •

### í™˜ê²½ë³€ìˆ˜ ì„¤ì • (.env)
```env
# LLM ëª¨ë¸ ì„¤ì •
MODEL_NAME=LGAI-EXAONE/EXAONE-3.5-7.8B-Instruct
MODEL_CACHE_DIR=./model_cache
TORCH_DTYPE=bfloat16
DEVICE_MAP=auto

# ì„œë²„ ì„¤ì •
HOST=0.0.0.0
PORT=8001
DEBUG=false
LOG_LEVEL=INFO

# GPU ì„¤ì •
CUDA_VISIBLE_DEVICES=0
LOW_CPU_MEM_USAGE=true

# TMDB API
TMDB_API_KEY=fbdca01cbe9008fcc8e7fd7dd6c1ba9c

# ngrok ê³ ì • ë„ë©”ì¸ (ì„ íƒì‚¬í•­)
NGROK_DOMAIN=your-domain.ngrok-free.app
```

### ìˆ˜ë™ ì‹¤í–‰ ë°©ë²•
```bash
# 1. í™˜ê²½ë³€ìˆ˜ ë¡œë“œ
source .env

# 2. LLM ì„œë²„ ì‹¤í–‰
python run.py

# 3. ngrok í„°ë„ (ë³„ë„ í„°ë¯¸ë„)
ngrok http 8001
```

## ğŸ“¡ API ì—”ë“œí¬ì¸íŠ¸

### í—¬ìŠ¤ ì²´í¬
```bash
# ë¡œì»¬
curl http://localhost:8001/health

# ngrok URL
curl https://your-ngrok-url.app/health
```

### LLM ìì—°ì–´ ì²˜ë¦¬
```bash
curl -X POST https://your-ngrok-url.app/llm \
  -H "Content-Type: application/json" \
  -d '{
    "message": "2024ë…„ ì•¡ì…˜ ì˜í™” ì¶”ì²œí•´ì¤˜"
  }'
```

**ì‘ë‹µ ì˜ˆì‹œ:**
```json
{
  "success": true,
  "parameters": {
    "with_genres": [28],
    "primary_release_date.gte": "2024-01-01",
    "primary_release_date.lte": "2024-12-31",
    "language": "ko-KR",
    "sort_by": "popularity.desc"
  },
  "confidence": 0.95,
  "method": "exaone_model"
}
```

### í…ŒìŠ¤íŠ¸ íŒŒì‹±
```bash
curl -X POST https://your-ngrok-url.app/test-parse \
  -H "Content-Type: application/json" \
  -d '{
    "message": "ë´‰ì¤€í˜¸ ê°ë…ì˜ ìµœì‹  ìŠ¤ë¦´ëŸ¬ ì˜í™”"
  }'
```

## ğŸ” ëª¨ë‹ˆí„°ë§ ë° ë””ë²„ê¹…

### GPU ìƒíƒœ í™•ì¸
```bash
# GPU ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰
nvidia-smi

# ì‹¤ì‹œê°„ ëª¨ë‹ˆí„°ë§
watch -n 1 nvidia-smi
```

### ì„œë²„ ë¡œê·¸ í™•ì¸
```bash
# ì‹¤ì‹œê°„ ë¡œê·¸
tail -f nohup.out

# íŠ¹ì • ë¡œê·¸ í•„í„°ë§
grep "ERROR" nohup.out
```

### í”„ë¡œì„¸ìŠ¤ ê´€ë¦¬
```bash
# ì‹¤í–‰ ì¤‘ì¸ Python í”„ë¡œì„¸ìŠ¤ í™•ì¸
ps aux | grep python

# í¬íŠ¸ ì‚¬ìš© í™•ì¸
lsof -i :8001

# í”„ë¡œì„¸ìŠ¤ ì¢…ë£Œ
kill -9 <PID>
```

## ğŸŒ ngrok ê³ ê¸‰ ì„¤ì •

### ê³ ì • ë„ë©”ì¸ ì‚¬ìš©
```bash
# ngrok ìœ ë£Œ ê³„ì •ì˜ ê³ ì • ë„ë©”ì¸
ngrok http 8001 --domain=your-domain.ngrok-free.app
```

### ngrok ì„¤ì • íŒŒì¼
```yaml
# ~/.ngrok2/ngrok.yml
version: "2"
authtoken: YOUR_TOKEN
tunnels:
  llm-server:
    addr: 8001
    proto: http
    domain: your-domain.ngrok-free.app
```

### í„°ë„ ì‹¤í–‰
```bash
# ì„¤ì • íŒŒì¼ ê¸°ë°˜ ì‹¤í–‰
ngrok start llm-server
```

## âš ï¸ íŠ¸ëŸ¬ë¸”ìŠˆíŒ…

### 1. GPU ë©”ëª¨ë¦¬ ë¶€ì¡±
```bash
# GPU ë©”ëª¨ë¦¬ ì •ë¦¬
python -c "
import torch
torch.cuda.empty_cache()
print('GPU ë©”ëª¨ë¦¬ ì •ë¦¬ ì™„ë£Œ')
"

# ëª¨ë¸ precision ë³€ê²½
export TORCH_DTYPE=float16
```

### 2. ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨
```bash
# ìˆ˜ë™ ë‹¤ìš´ë¡œë“œ
python -c "
from transformers import AutoTokenizer, AutoModelForCausalLM
tokenizer = AutoTokenizer.from_pretrained('LGAI-EXAONE/EXAONE-3.5-7.8B-Instruct')
print('í† í¬ë‚˜ì´ì € ë‹¤ìš´ë¡œë“œ ì™„ë£Œ')
"
```

### 3. í¬íŠ¸ ì¶©ëŒ
```bash
# ì‚¬ìš© ì¤‘ì¸ í”„ë¡œì„¸ìŠ¤ ì¢…ë£Œ
sudo lsof -ti:8001 | xargs kill -9

# ë‹¤ë¥¸ í¬íŠ¸ ì‚¬ìš©
export PORT=8011
./start_llm_server.sh
```

### 4. ngrok ì—°ê²° ì‹¤íŒ¨
```bash
# ngrok ìƒíƒœ í™•ì¸
ngrok config check

# í† í° ì¬ì„¤ì •
ngrok config add-authtoken NEW_TOKEN
```

## ğŸ“ Cloudtype ì—°ë™

### í™˜ê²½ë³€ìˆ˜ ì—…ë°ì´íŠ¸
Cloudtype ëŒ€ì‹œë³´ë“œì—ì„œ ë‹¤ìŒ í™˜ê²½ë³€ìˆ˜ ì¶”ê°€:

```env
# ngrok URL ì„¤ì •
LLM_SERVER_URL=https://your-ngrok-url.app

# ê¸°íƒ€ ì„¤ì •
ENVIRONMENT=production
USE_LOCAL_DATA=true
```

### ì—°ë™ í…ŒìŠ¤íŠ¸
```bash
# Cloudtypeì—ì„œ LLM ì„œë²„ í˜¸ì¶œ í…ŒìŠ¤íŠ¸
curl -X POST https://your-cloudtype-app.app/api/movies/recommend \
  -H "Content-Type: application/json" \
  -d '{
    "message": "2024ë…„ ì•¡ì…˜ ì˜í™” ì¶”ì²œí•´ì¤˜"
  }'
```

## ğŸ¯ ì „ì²´ í”Œë¡œìš°

```
[ì‚¬ìš©ì ìš”ì²­] 
    â†“
[Spring Frontend] 
    â†“
[Cloudtype Proxy Server] 
    â†“
[ngrok Tunnel] 
    â†“
[Linux GPU LLM Server] 
    â†“
[EXAONE 3.5-7.8B ëª¨ë¸]
    â†“
[TMDB API íŒŒë¼ë¯¸í„° ë³€í™˜]
    â†“
[ê²°ê³¼ ë°˜í™˜]
```

## ğŸ”„ ìë™ ì‹œì‘ ì„¤ì •

### systemd ì„œë¹„ìŠ¤ ìƒì„±
```bash
# ì„œë¹„ìŠ¤ íŒŒì¼ ìƒì„±
sudo nano /etc/systemd/system/opus-llm.service
```

```ini
[Unit]
Description=OpusCine LLM Server
After=network.target

[Service]
Type=simple
User=your-username
WorkingDirectory=/path/to/llm-server
Environment=PATH=/home/your-username/anaconda3/envs/gpu/bin
ExecStart=/home/your-username/anaconda3/envs/gpu/bin/python run.py
Restart=always

[Install]
WantedBy=multi-user.target
```

### ì„œë¹„ìŠ¤ í™œì„±í™”
```bash
sudo systemctl daemon-reload
sudo systemctl enable opus-llm
sudo systemctl start opus-llm
```

## ğŸ“ˆ ì„±ëŠ¥ ìµœì í™”

### GPU ë©”ëª¨ë¦¬ ìµœì í™”
```env
# .env íŒŒì¼ì— ì¶”ê°€
LOW_CPU_MEM_USAGE=true
LOAD_IN_8BIT=true
TORCH_COMPILE=true
```

### ë³‘ë ¬ ì²˜ë¦¬ ì„¤ì •
```env
MAX_CONCURRENT_REQUESTS=4
REQUEST_TIMEOUT=30
```

ì´ì œ WSLì—ì„œ `conda activate gpu` í›„ `./gpu_server_setup.sh`ë¥¼ ì‹¤í–‰í•˜ì—¬ ì™„ì „í•œ LLM GPU ì„œë²„ë¥¼ ì„¤ì •í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤! ğŸš€ 