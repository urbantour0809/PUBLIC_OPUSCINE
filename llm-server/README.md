# π¤– OpusCine LLM Server

EXAONE 3.5-7.8B λ¨λΈμ„ μ‚¬μ©ν• μμ—°μ–΄ μ²λ¦¬ μ„λ²„μ…λ‹λ‹¤.

## π“‹ μ”κµ¬μ‚¬ν•­

### ν•λ“μ›¨μ–΄
- **GPU**: NVIDIA GPU (μµμ† 16GB VRAM κ¶μ¥)
- **RAM**: 32GB μ΄μƒ κ¶μ¥
- **Storage**: 50GB μ΄μƒ μ—¬μ  κ³µκ°„

### μ†ν”„νΈμ›¨μ–΄
- Python 3.11+
- CUDA 11.8+
- PyTorch 2.1.0+

## π€ μ„¤μΉ λ° μ‹¤ν–‰

### 1. μμ΅΄μ„± μ„¤μΉ

```bash
# Python κ°€μƒν™κ²½ μƒμ„± (κ¶μ¥)
python -m venv llm_env
source llm_env/bin/activate  # Linux/Mac
# λλ”
llm_env\Scripts\activate     # Windows

# μμ΅΄μ„± μ„¤μΉ
pip install -r requirements.txt
```

### 2. ν™κ²½ μ„¤μ •

```bash
# .env νμΌ λ³µμ‚¬ λ° μμ •
cp .env.example .env
nano .env  # ν•„μ”μ‹ μ„¤μ • μμ •
```

**μ£Όμ” ν™κ²½λ³€μ:**
```env
MODEL_NAME=LGAI-EXAONE/EXAONE-3.5-7.8B-Instruct
CUDA_VISIBLE_DEVICES=0
PORT=8001
```

### 3. μ„λ²„ μ‹¤ν–‰

```bash
# λ°©λ²• 1: run.py μ¤ν¬λ¦½νΈ μ‚¬μ© (κ¶μ¥)
python run.py

# λ°©λ²• 2: μ§μ ‘ μ‹¤ν–‰
python app.py

# λ°©λ²• 3: uvicorn μ§μ ‘ μ‚¬μ©
uvicorn app:app --host 0.0.0.0 --port 8001
```

### 4. μƒνƒ ν™•μΈ

```bash
# ν—¬μ¤ μ²΄ν¬
curl http://localhost:8001/health

# λ¨λΈ μ •λ³΄ ν™•μΈ
curl http://localhost:8001/
```

## π“΅ API μ—”λ“ν¬μΈνΈ

### κΈ°λ³Έ μ •λ³΄
- **κΈ°λ³Έ URL**: `http://localhost:8001`
- **Content-Type**: `application/json`

### μ£Όμ” μ—”λ“ν¬μΈνΈ

#### 1. ν—¬μ¤ μ²΄ν¬
```
GET /
GET /health
```

#### 2. μμ—°μ–΄ μ²λ¦¬
```
POST /llm
Content-Type: application/json

{
  "message": "2024λ…„ μ•΅μ… μν™” μ¶”μ²ν•΄μ¤"
}
```

**μ‘λ‹µ μμ‹:**
```json
{
  "success": true,
  "parameters": {
    "with_genres": [28],
    "primary_release_date.gte": "2024-01-01",
    "primary_release_date.lte": "2024-12-31",
    "language": "ko-KR",
    "sort_by": "popularity.desc"
  },
  "confidence": 0.95,
  "method": "exaone_model"
}
```

#### 3. ν…μ¤νΈ νμ‹±
```
POST /test-parse
Content-Type: application/json

{
  "message": "λ΄‰μ¤€νΈ κ°λ…μ μµμ‹  μν™”"
}
```

## π”§ μ„¤μ • μµμ…

### λ¨λΈ μ„¤μ •
```env
MODEL_NAME=LGAI-EXAONE/EXAONE-3.5-7.8B-Instruct
MODEL_CACHE_DIR=./model_cache
TORCH_DTYPE=bfloat16
DEVICE_MAP=auto
```

### μƒμ„± νλΌλ―Έν„°
```env
MAX_NEW_TOKENS=512
TEMPERATURE=0.1
DO_SAMPLE=false
```

### μ„λ²„ μ„¤μ •
```env
HOST=0.0.0.0
PORT=8001
DEBUG=false
LOG_LEVEL=INFO
```

## π³ Docker μ‹¤ν–‰

### Dockerfile λΉλ“
```bash
docker build -t opus-llm-server .
```

### μ»¨ν…μ΄λ„ μ‹¤ν–‰
```bash
docker run -d \
  --name opus-llm \
  --gpus all \
  -p 8001:8001 \
  -v $(pwd)/model_cache:/app/model_cache \
  -e CUDA_VISIBLE_DEVICES=0 \
  opus-llm-server
```

## π ngrok μ„¤μ • (μ™Έλ¶€ μ ‘κ·Όμ©)

### ngrok μ„¤μΉ λ° μ„¤μ •
```bash
# ngrok μ„¤μΉ
curl -s https://ngrok-agent.s3.amazonaws.com/ngrok.asc | sudo tee /etc/apt/trusted.gpg.d/ngrok.asc >/dev/null
echo "deb https://ngrok-agent.s3.amazonaws.com buster main" | sudo tee /etc/apt/sources.list.d/ngrok.list
sudo apt update && sudo apt install ngrok

# ν† ν° μ„¤μ •
ngrok config add-authtoken YOUR_NGROK_TOKEN
```

### ν„°λ„ μ‹μ‘
```bash
# μ„λ²„ μ‹¤ν–‰ ν›„ λ³„λ„ ν„°λ―Έλ„μ—μ„
ngrok http 8001
```

## π“ λ¨λ‹ν„°λ§

### μ‹μ¤ν… λ¦¬μ†μ¤ ν™•μΈ
```bash
# GPU λ©”λ¨λ¦¬ μ‚¬μ©λ‰
nvidia-smi

# ν”„λ΅μ„Έμ¤ ν™•μΈ
ps aux | grep python

# λ΅κ·Έ ν™•μΈ
tail -f llm_server.log
```

### API μƒνƒ ν™•μΈ
```bash
# λ¨λΈ λ΅λ“ μƒνƒ
curl http://localhost:8001/health

# μ‘λ‹µ μ‹κ°„ ν…μ¤νΈ
time curl -X POST http://localhost:8001/llm \
  -H "Content-Type: application/json" \
  -d '{"message": "ν…μ¤νΈ λ©”μ‹μ§€"}'
```

## β οΈ νΈλ¬λΈ”μν…

### 1. GPU λ©”λ¨λ¦¬ λ¶€μ΅±
```bash
# GPU λ©”λ¨λ¦¬ ν™•μΈ
nvidia-smi

# ν•΄κ²° λ°©λ²•:
# - CUDA_VISIBLE_DEVICESλ΅ νΉμ • GPU μ§€μ •
# - batch_size μ¤„μ΄κΈ°
# - λ¨λΈ precision λ³€κ²½ (float16)
```

### 2. λ¨λΈ λ‹¤μ΄λ΅λ“ μ‹¤ν¨
```bash
# μλ™ λ‹¤μ΄λ΅λ“
python -c "
from transformers import AutoTokenizer, AutoModelForCausalLM
tokenizer = AutoTokenizer.from_pretrained('LGAI-EXAONE/EXAONE-3.5-7.8B-Instruct')
model = AutoModelForCausalLM.from_pretrained('LGAI-EXAONE/EXAONE-3.5-7.8B-Instruct')
"
```

### 3. ν¬νΈ μ¶©λ
```bash
# ν¬νΈ μ‚¬μ© ν™•μΈ
lsof -i :8001

# λ‹¤λ¥Έ ν¬νΈ μ‚¬μ©
export PORT=8011
python run.py
```

## π“ μ§€μ›

λ¬Έμ κ°€ λ°μƒν•λ©΄ λ‹¤μμ„ ν™•μΈν•μ„Έμ”:

1. GPU λ“λΌμ΄λ²„ λ° CUDA λ²„μ „
2. Python ν¨ν‚¤μ§€ λ²„μ „ νΈν™μ„±  
3. λ¨λΈ λ‹¤μ΄λ΅λ“ μƒνƒ
4. ν™κ²½λ³€μ μ„¤μ •
5. λ°©ν™”λ²½ λ° ν¬νΈ μ„¤μ •

## π“ λΌμ΄μ„Όμ¤

μ΄ ν”„λ΅μ νΈλ” MIT λΌμ΄μ„Όμ¤λ¥Ό λ”°λ¦…λ‹λ‹¤. 